# HukukBul Platformu

HukukBul, TÃ¼rkÃ§e hukuk bilgisi arayanlar iÃ§in hazÄ±rlanmÄ±ÅŸ Ã§ok katmanlÄ± bir yapay zekÃ¢ destekli danÄ±ÅŸman prototipidir. Ana hedef; kullanÄ±cÄ± sorularÄ±nÄ± mevzuat ve emsal kararlarla iliÅŸkilendirerek okunabilir cevaplar Ã¼retmek ve aynÄ± zamanda profesyonel kullanÄ±cÄ±lar iÃ§in daha teknik Ã§Ä±ktÄ±larÄ±n Ã¶nÃ¼nÃ¼ aÃ§maktÄ±r.

## âœ¨ Ã–ne Ã‡Ä±kanlar
- Next.js tabanlÄ± modern sohbet arayÃ¼zÃ¼
- Fastify ile hafif API katmanÄ± ve FastAPI ile LLM kÃ¶prÃ¼sÃ¼
- vLLM Ã¼zerinde Ã§alÄ±ÅŸan Qwen serisi modeller (varsayÄ±lan 1.5B instruct)
- Postgres, Qdrant ve Redis ile hazÄ±r veri/embedding altyapÄ± iskeleti
- Docker Compose ile tek komutla ayaÄŸa kalkabilen modÃ¼ler mimari

## Mimari Genel BakÄ±ÅŸ
```
                +-------------------+
                |   Next.js UI      |
                |   (frontend)      |
                +---------+---------+
                          |
                          v
+-------------------------+-------------------------+
|              Fastify API (api)                    |
+-------------------------+-------------------------+
                          |
                          v
                +-------------------+
                | FastAPI LLM Proxy |
                |      (ai)         |
                +---------+---------+
                          |
                          v
                +-------------------+
                |   vLLM + Qwen     |
                |      (vllm)       |
                +-------------------+
```

Destek servisleri:
- **postgres**: ileride mevzuat verisinin saklanmasÄ± iÃ§in hazÄ±r
- **qdrant**: vektÃ¶r arama altyapÄ±sÄ±
- **redis**: hÄ±zlÄ± cache / kuyruk iÅŸlemleri
- **ingest**: henÃ¼z iskelet; belge iÅŸleme pipelineâ€™Ä± burada geliÅŸtirilecek
- **nginx**: frontend ve API iÃ§in tek giriÅŸ noktasÄ± (`http://localhost:8088`)

## Gereksinimler
- Docker ve Docker Compose (GPU eriÅŸimi iÃ§in NVIDIA Container Toolkit)
- Ortalama 8 GB VRAMâ€™li GPU (Qwen 7B iÃ§in daha fazla gerekebilir)
- Ä°lk Ã§alÄ±ÅŸtÄ±rmada internet baÄŸlantÄ±sÄ± (model dosyalarÄ±nÄ± indirmek iÃ§in)

## HÄ±zlÄ± BaÅŸlangÄ±Ã§
```bash
# servislere yeni baÅŸladÄ±ysanÄ±z
./start.sh

# durum kontrolÃ¼
docker compose ps

# log takip (LLM uzun sÃ¼rede aÃ§Ä±labilir)
docker logs -f hukukbul-vllm-1
```

ArayÃ¼z: `http://localhost:8088`

API test Ã¶rneÄŸi:
```bash
curl -s \
  -X POST http://localhost:8088/api/ask \
  -H 'Content-Type: application/json' \
  -d '{"query":"BoÅŸanma davasÄ±nda nafaka nasÄ±l belirlenir?"}'
```

## YapÄ±landÄ±rma Ä°puÃ§larÄ±
- `docker-compose.yml` iÃ§inde:
  - `LLM_MODEL` ile farklÄ± Qwen modelleri seÃ§ilebilir
  - `--max-model-len`, `--gpu-memory-utilization` gibi argÃ¼manlarla GPU kullanÄ±mÄ± ayarlanabilir
- `.env` (oluÅŸturulmalÄ±): Postgres kullanÄ±cÄ± adÄ±, ÅŸifresi ve portlar iÃ§in gerekli

### Model DosyalarÄ±
Ä°lk Ã§alÄ±ÅŸtÄ±rmada HuggingFace Ã¼zerinden model aÄŸÄ±rlÄ±klarÄ± `models/hub` klasÃ¶rÃ¼ne indirilir. Bu indirmenin her makinede yalnÄ±zca bir kez yapÄ±lmasÄ± gerekir; klasÃ¶r silinirse tekrar indirilir. BÃ¼yÃ¼k modeller iÃ§in disk alanÄ±nÄ± kontrol etmeyi unutmayÄ±n.

## GeliÅŸtirici AkÄ±ÅŸÄ±
1. **Frontend**: `frontend/app/page.tsx` Ã¼zerinde Next.js bileÅŸenleri ile sohbet arayÃ¼zÃ¼.
2. **API (Node)**: `api/server.js` Fastify ile `/ask` uÃ§ noktasÄ±nÄ± vLLM servisine yÃ¶nlendirir.
3. **AI Servisi (Python)**: `ai/main.py` gelen sorularÄ± LLMâ€™e OpenAI uyumlu istek olarak aktarÄ±r.
4. **LLM (vLLM)**: `vllm` servisi GPU Ã¼zerinden modeli Ã§alÄ±ÅŸtÄ±rÄ±r.
5. **Ingest Pipeline**: `ingest/worker.py` ileride veri toplama ve embedding sÃ¼reci iÃ§in hazÄ±r bekler.

GÃ¼ncelleme veya geliÅŸtirme sÄ±rasÄ±nda:
```bash
# tek servis yeniden baÅŸlatma
docker compose restart api

# loglarÄ± yakÄ±ndan takip etmek iÃ§in
docker logs -f hukukbul-ai-1
```

## Sorun Giderme
| Belirti | Muhtemel Sebep | Ã‡Ã¶zÃ¼m |
| --- | --- | --- |
| `/api/ask` yanÄ±tÄ± â€œAll connection attempts failedâ€ | vLLM henÃ¼z hazÄ±r deÄŸil | `docker logs -f hukukbul-vllm-1` ile yÃ¼kleme sÃ¼recini bekleyin |
| vLLM konteyneri GPU belleÄŸi hatasÄ± ile Ã§Ä±kÄ±yor | Model VRAMâ€™e sÄ±ÄŸmÄ±yor | `docker-compose.yml` iÃ§inde daha kÃ¼Ã§Ã¼k model veya daha dÃ¼ÅŸÃ¼k `--max-model-len` ayarlayÄ±n |
| Model her baÅŸlatmada yeniden indiriliyor | `models/hub` klasÃ¶rÃ¼ silinmiÅŸ | KlasÃ¶rÃ¼ koruyun veya paylaÅŸÄ±lan bir disk kullanÄ±n |

## Dizin YapÄ±sÄ±
```
.
â”œâ”€â”€ ai/            # FastAPI tabanlÄ± LLM proxy
â”œâ”€â”€ api/           # Fastify API katmanÄ±
â”œâ”€â”€ frontend/      # Next.js sohbet arayÃ¼zÃ¼
â”œâ”€â”€ ingest/        # Ä°leride kullanÄ±lacak belge pipeline iskeleti
â”œâ”€â”€ models/        # HuggingFace cache (LLM aÄŸÄ±rlÄ±klarÄ±)
â”œâ”€â”€ deploy/nginx/  # Reverse proxy ayarlarÄ±
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ start.sh / restart.sh
â””â”€â”€ README.md
```

## Yol HaritasÄ±
- Ingest pipelineâ€™Ä±nÄ±n tamamlanmasÄ± ve Qdrant entegrasyonu
- Mevzuat ve emsal karar referanslarÄ±nÄ±n sohbet Ã§Ä±ktÄ±sÄ±na eklenmesi
- KullanÄ±cÄ± modlarÄ±na gÃ¶re (Halk / Pro) farklÄ± yanÄ±t stratejilerinin zenginleÅŸtirilmesi
- GeliÅŸmiÅŸ loglama ve gÃ¶zlemlenebilirlik eklenmesi

---

Sorular veya geri bildirimler iÃ§in issues aÃ§abilir ya da doÄŸrudan proje yÃ¶neticisiyle iletiÅŸime geÃ§ebilirsiniz. Keyifli keÅŸifler! ğŸš€
